def main():
    total_files = 0
    total_rows  = 0

    for s in SOURCES:
        key = s["key"]
        d   = s["dir"]
        if not d.exists():
            print(f"[INFO] source '{key}' dir not exists: {d}")
            continue

        # 只抓 .csv，依修改時間排序（舊到新）
        csvs = sorted([p for p in d.glob("*.csv")], key=lambda x: x.stat().st_mtime)
        if not csvs:
            print(f"[INFO] no csvs for source '{key}' in {d}")
            continue

        print(f"[INFO] === {key} === files={len(csvs)} dir={d}")

        for p in csvs:
            try:
                # 1) 解析單一檔案
                df = parse_csv_generic(p, key)
                print(f"[PARSE] {key}: {p.name} -> {len(df)} rows")
                total_files += 1
                if df.empty:
                    continue

                # 2) 地理補點
                try:
                    df = apply_geo_fixes_df(df)
                except Exception as ge:
                    print(f"[WARN] geo fixes skipped for {p.name}: {ge}")

                # 3) 寫入（write_to_influx 內已有分批/清洗）
                ok = write_to_influx(df)
                if ok:
                    total_rows += len(df)

            except Exception as e:
                print(f"[WARN] parse/write failed {key}: {p.name} ({e})")

    print(f"[SUMMARY] files={total_files}, rows_written~={total_rows}")








def write_to_influx(df: pd.DataFrame) -> bool:
    if df.empty:
        print("[INFO] nothing to write.")
        return True
    try:
        safe_numeric_fields = ["bytes_in","bytes_out","src_port","dst_port","flex_num1","count"]

        # —— 只留安全欄位（時間 + tags + 數值欄位）——
        keep_cols = (["event_ts"]
                     + [c for c in TAG_COLS if c in df.columns]
                     + [c for c in safe_numeric_fields if c in df.columns])
        df = df.loc[:, [c for c in keep_cols if c in df.columns]].copy()

        # 數值欄位轉數字
        for c in safe_numeric_fields:
            if c in df.columns:
                df[c] = pd.to_numeric(df[c], errors="coerce")

        # —— 保證至少有一個 field 可寫 ——（⚠️取代原本的 dropna）
        if not any(c in df.columns for c in safe_numeric_fields):
            df["count"] = 1.0
        else:
            # 這些列的所有數值欄位都是 NaN → 給它 count=1 當保底
            mask_all_nan = df[[c for c in safe_numeric_fields if c in df.columns]].isna().all(axis=1)
            if mask_all_nan.any():
                df.loc[mask_all_nan, "count"] = 1.0

        # 時間正規化（仍建議保留）
        df["event_ts"] = pd.to_datetime(df["event_ts"], errors="coerce", utc=True)
        df = df[~df["event_ts"].isna()].copy()

        # Influx 友善清洗（一定保留）
        df = _sanitize_for_influx(df)

        # —— 分批同步寫入（最穩）——
        from math import ceil
        BATCH_SIZE = 10_000
        with InfluxDBClient(url=INFLUX_URL, org=ORG, token=TOKEN, timeout=300_000) as cli:
            w = cli.write_api(write_options=SYNCHRONOUS)
            total = len(df); batches = ceil(total / BATCH_SIZE)
            for i in range(batches):
                chunk = df.iloc[i*BATCH_SIZE:(i+1)*BATCH_SIZE]
                w.write(
                    bucket=BUCKET,
                    org=ORG,
                    record=chunk,
                    data_frame_measurement_name=MEAS,
                    data_frame_tag_columns=[c for c in TAG_COLS if c in chunk.columns],
                    data_frame_timestamp_column="event_ts",
                )
                print(f"[OK] wrote chunk {i+1}/{batches} rows={len(chunk)}")
        print(f"[OK] wrote total {len(df)} rows to {BUCKET}/{MEAS}")
        return True
    except Exception as e:
        import traceback; traceback.print_exc()
        print(f"[ERROR] write influx failed: {e}")
        return False
